{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration - Table Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_states = 10 # Number of states\n",
    "n_actions = 4 # Number of actions\n",
    "gamma = 0.9 # Discount Factor\n",
    "tolerance = 0.00001 # Convergence criteria\n",
    "max_iterations = 100 # Maximum number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set rewards R(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rewards = np.zeros([n_states, n_actions])\n",
    "rewards[-1] = 1 # Goal state\n",
    "rewards[-2] = -1 # Penalty state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transition_prob = np.random.random([n_states,n_actions,n_states])\n",
    "s = transition_prob.sum(axis=-1)\n",
    "transition_prob = transition_prob/np.repeat(s, n_states).reshape([n_states, n_actions, n_states]) # Normalization\n",
    "transition_prob[-1] = 0 # Make goal state terminal\n",
    "transition_prob[-2] = 0 # Make penalty state terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = np.random.randint(n_actions, size=n_states)\n",
    "state_values = np.zeros(n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration through Bellman updates until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Initial Random Policy', array([1, 3, 1, 1, 1, 0, 2, 3, 1, 3]))\n",
      "('Learned Policy', array([3, 1, 3, 3, 1, 0, 1, 2, 0, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print('Initial Random Policy', policy)\n",
    "itr = 0\n",
    "while itr < max_iterations:\n",
    "    itr += 1\n",
    "    for s in range(n_states):\n",
    "        state_values[s] = rewards[s, policy[s]] + gamma*np.dot(transition_prob[s, policy[s]], state_values) # Bellman Update\n",
    "    new_policy = np.zeros(n_states)\n",
    "    for s in range(n_states):\n",
    "        action_values = np.zeros([n_actions])\n",
    "        for a in range(n_actions):\n",
    "            action_values[a] = rewards[s, a] + gamma*np.dot(transition_prob[s, a], state_values)\n",
    "        new_policy[s] = np.argmax(action_values)\n",
    "    new_policy = new_policy.astype(np.int32)\n",
    "    if np.array_equal(new_policy, policy):\n",
    "        break\n",
    "    policy = new_policy.copy()\n",
    "\n",
    "print('Learned Policy', policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
